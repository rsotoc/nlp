{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![agents](images/header.jpg)\n",
    "# Limpieza y tokenización\n",
    "### Ramón Soto C. [(rsotoc@moviquest.com)](mailto:rsotoc@moviquest.com/)\n",
    "[ver en nbviewer](http://nbviewer.ipython.org/github/rsotoc/nlp/blob/master/2.%20Tokenización.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenización\n",
    "\n",
    "El primer paso para analizar el mensaje contenido en un documento de texto es la **tokenización**, es decir, la segmentación del texto original (en bruto) para obtener un conjunto de palabras y símbolos que puedan ser analizados como entidades con significado. \n",
    "\n",
    "La definición de *token*, particularmente en su uso en informática, no es muy preciso. Sin embargo, hay un consenso establecido en entender un *token* como la ocurrencia individual de una unidad mínima de lenguaje. Esta unidad puede pertenecer a una diversidad de *tipos* de elementos gramaticales y su única restricción es que sea un bloque indivisible de texto útil para transmitir un mensaje. En muchos casos, la tokenización se realiza como parte del análisis léxico, de tal manera que conforme se van obteniendo los tokens se les va asignando una categoría. En este curso y por razones prácticas, limitaremos la tokenización a la segmentación del texto en unidades sin categorizar, dejando esta tarea adicional para la fase de análisis léxico.\n",
    "\n",
    "![](images/nlp02b.png)\n",
    "\n",
    "La pertinencia de esta división depende mucho del lenguaje y del contexto del mensaje que se está analizando.\n",
    "\n",
    "![](images/ideo_tok.png)\n",
    "\n",
    "En el caso de lenguajes como el español y el inglés, la estructura del lenguaje escrito es predominantemente alfabética con el uso de logogramas para representación alternativa/compacta de números y para símbolos ([sistemas de escritura](https://en.wikipedia.org/wiki/Writing_system)). En este caso, la segmentación de un texto en *tokens* es relativamente simple, utilizando caracteres de demarcación como pueden ser los espacios en blanco y los signos de puntuación.\n",
    "\n",
    "Utilizando la biblioteca [nltk (*Natural language toolkit*)](http://www.nltk.org) de python, por ejemplo, podemos descomponer el siguiente [texto](https://www.youtube.com/watch?v=CJkBsVGRNF0):\n",
    "\n",
    "> Grave desencanto te ha dado, a resultas, uh, uh, <br>\n",
    "del cual te dedicas ahora a las ciencias ocultas.  <br>\n",
    "Tú que ya eras torpe de racionalista,  <br>\n",
    "no lo tienes fácil, Satanás te asista.  <br>\n",
    "Siendo sólo fea, siendo sólo arpía,  <br>\n",
    "nunca serás bruja de categoría.  <br>\n",
    "\n",
    "en frases..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "# Modificar la longitud de caracteres en una celda que se imprimirán \n",
    "pd.options.display.max_colwidth = 150 \n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = [r\"\"\"Grave  te ha dado, a resultas, uh, uh, \n",
    "del cual te dedicas ahora a las ciencias ocultas. \n",
    "Tú que ya eras torpe de racionalista, \n",
    "no lo tienes fácil, Satanás te asista. \n",
    "Siendo sólo fea, siendo sólo arpía, \n",
    "nunca serás bruja de categoría.\"\"\"]\n",
    "print(raw_text[0], \"\\n\")\n",
    "\n",
    "frases = sent_tokenize(raw_text[0])\n",
    "print(frases, \"\\n\")\n",
    "\n",
    "#Convertir a Dataframe, para manipular más adecuadamente\n",
    "df_frases = pd.DataFrame(frases)\n",
    "df_frases.columns = [\"Frase\"]\n",
    "display(df_frases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... o en palabras+símbolos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pasar el valor en la primera celda del dataframe al tokenizador\n",
    "palabras = word_tokenize(df_frases.iloc[0,0]) \n",
    "df_palabras = pd.DataFrame(palabras)\n",
    "df_palabras.columns = [\"Palabra\"]\n",
    "display(df_palabras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ ](images/blank.png)\n",
    "## Limpieza del texto en bruto\n",
    "\n",
    "Un problema que enfrentamos usualmente en el reconocimiento de patrones es que los datos suelen venir \"sucios\", es decir, pueden incluir valores falsos o inusuales que afectan la eficacia de los métodos. Este problema es particularmente importante en el caso del análisis automático de textos. La razón de ello es que, en una gran mayoría de los casos, los documentos de texto disponibles se escriben escritos de manera \"*libre*\", sin sujetarse a estructuras formales.\n",
    "\n",
    "[![](images/nlp03.png)](https://squirro.com/2017/02/16/unstructured-data-waking-giant/)\n",
    "\n",
    "Estos textos suelen contener errores de escritura (*typos*) y de gramática, palabras de algún argot, contenido no deseado como URLs o etiquetas de marcaje. Considérese, por ejemplo, la siguiente revisión (alterada -\\_-) de una película:\n",
    "\n",
    "> I thought that Ice Age was an excellent movie! As a woman of 30, with no children, I really &amp;lt;3 these animated movies. They're fantastic! &lt;br />&lt;br />&lt;b>Sid&lt;/b> is the best character I have seen in some time, better than &lt;i>Bartok&lt;/i> in Anastasia &amp;amp; even more humorous than &lt;i>Melman&lt;/i> in Madagascar. &lt;br />&lt;br />My favourite scene is the part where Sid says \\Oh, oh, oh, I love this game!\\\" and Sid and Manny continue to figure out what the squirrel is trying to tell them about the \\\"tigers\\\"...\\\"Pack of wolves, pack of bears, pack of fleas, pack of whiskers, pack of noses, pack a derm?, pack of lies, pack of troubles, pack a wallop, pack of birds, pack of flying fish...\\\" or however that part goes! ROFL, that's so funny LMAO! &lt;br />&lt;br />If anyone has any tips on where to buy the director's cut version, please email me at darkmaster666@yahoo.com.\n",
    "\n",
    "Al repetir los ejercicios de tokenización obtenemos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text2 = [r\"\"\"I thought that Ice Age was an excellent movie! As a woman of 30, with no children, I really\n",
    "            &lt;3 these animated movies. They're fantastic! <br /><br /><b>Sid</b> is the best character I have \n",
    "            seen in some time, better than <i>Bartok</i> in Anastasia &amp; even more humorous than <i>Melman</i> \n",
    "            in Madagascar. <br /><br />My favourite scene is the part where Sid says \\Oh, oh, oh, I love this \n",
    "            game!\\\" and Sid \"\\ and Manny continue to figure out what the squirrel is trying to tell them about \n",
    "            the \\\"tigers\\\"... \\\"Pack of wolves, pack of bears, pack of fleas, pack of whiskers, pack of noses, \n",
    "            pack a derm?, pack of lies, pack of troubles, pack a wallop, pack of birds, pack of flying fish...\\\" \n",
    "            or however that part goes! ROFL, that's so funny LMAO! <br /><br />If anyone has any tips on where to \n",
    "            buy the director's cut version, please email me at darkmaster666@yahoo.com.\"\"\".replace('\\n',' ')]\n",
    "\n",
    "raw_text2[0] = ' '.join(raw_text2[0].split())\n",
    "print(raw_text2[0], \"\\n\")\n",
    "\n",
    "frases2 = sent_tokenize(raw_text2[0])\n",
    "df_frases2 = pd.DataFrame(frases2)\n",
    "df_frases2.columns = [\"Frase\"]\n",
    "display(df_frases2)\n",
    "\n",
    "palabras2 = word_tokenize(raw_text2[0][83:150]) # Pasar un segmento de la cadena al tokenizador\n",
    "df_palabras2 = pd.DataFrame(palabras2)\n",
    "df_palabras2.columns = [\"Palabra\"]\n",
    "display(df_palabras2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta segmentación podemos observar la presencia de *tokens* que son de poca utilidad para analizar el mensaje o que incluso son el resultado de una \"mala\" segmentación. Destaca el conjunto de tokens 2 al 5 en este dataframe (&amp;, lt, ;, 3) que debieran visualizarse como <3 e interpretarse como \"love\". También preocupa el token \"'re\" o los símbolos obtenidos de las etiquetas de marcaje.\n",
    "\n",
    "De esta manera, antes de proceder a tokenizar el texto conviene realizar algunas tareas de limpieza. \n",
    "\n",
    "#### 1. Texto en minúsculas\n",
    "La primera de ellas, que en una aproximación estándar no afecta la interpretación del texto y facilita, por otra parte, la comparación de tokens, consisten en transformar el texto a minúsculas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto1 = raw_text2[0].lower()\n",
    "print(texto1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Eliminación de URLs\n",
    "\n",
    "Si nos fijamos en la segmentación de las últimas palabras en este texto, observamos lo siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(raw_text2[0][825:])\n",
    "\n",
    "palabras3 = word_tokenize(raw_text2[0][824:])\n",
    "df_palabras3 = pd.DataFrame(palabras3)\n",
    "df_palabras3.columns = [\"Palabra\"]\n",
    "display(df_palabras3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por una parte, la dirección de correo electrónico aporta muy poco a la interpretación del mensaje (basta saber que la autora desea que se le envíe información) y por otra parte, la dirección (un solo objeto) fue descompuesto en 3 tokens. Usualmente esta es otra tarea de limpieza del texto: eliminar las URLs. Para ello, lo más simple es emplear expresiones regulares que reconozcan estas palabras y las eliminen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "texto2 = re.sub(r'[\\w\\.-]+@[\\w\\.-]+', \" \", texto1)\n",
    "texto2 = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', ' ',  texto2)\n",
    "print(texto2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Eliminación de etiquetas de marcaje\n",
    "\n",
    "A continuación, eliminamos las etiquetas de marcaje cuya función principal es agregar \"emotividad\" al mensaje. Esta actividad también puede realizarse mediante el uso de expresiones regulares, sin embargo, la biblioteca **BeautifulSoup** arroja muy buenos resultados y es de fácil uso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "texto3 = BeautifulSoup(texto2, \"lxml\").get_text() \n",
    "print(texto3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Eliminar signos de puntuación\n",
    "\n",
    "Los signos de puntuación tienen dos efectos negativos: 1) Se identifican como tokens independientes, haciendo más complejo el espacio de características, y 2) Dificulta segmentar una cadena (cuando no se utiliza una biblioteca especializada como *nltk.tokenize*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palabras3 = word_tokenize(texto3)\n",
    "print(\"Tokens diferentes en el texto completo (usando nltk.tokenize): \", \n",
    "      len( set(palabras3)), \"\\n\")\n",
    "\n",
    "puncList = [\".\", \";\", \":\", \"!\", \"?\", \"/\", \"\\\\\", \",\", \")\", \"(\", \"\\\"\"]\n",
    "for punc in puncList:\n",
    "    palabras3 = [word.replace(punc,'') for word in palabras3]\n",
    "print(\"Tokens diferentes en el texto sin signos de puntuación (usando nltk.tokenize): \", \n",
    "      len( set(palabras3)), \"\\n\")\n",
    "\n",
    "print(\"Tokens en el texto completo utilizando string.split, delimitando con espacios:\\n\",\n",
    "     texto3.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una elección común es eliminar los signos de puntuación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = texto3.split()\n",
    "for punc in puncList:\n",
    "    words = [word.replace(punc,'') for word in words]\n",
    "\n",
    "texto4 = \" \".join(words)\n",
    "print(texto4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una alternativa que rescata la estructura marcada por el punto, que delimita las oraciones en un texto, consiste en procesar por frases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "frases3 = sent_tokenize(texto3)\n",
    "df_frases3 = pd.DataFrame(frases3)\n",
    "df_frases3.columns = [\"Frase\"]\n",
    "\n",
    "for punc in puncList:\n",
    "    df_frases3[\"Frase\"] = [word.replace(punc,'') for word in df_frases3[\"Frase\"]]\n",
    "\n",
    "display(df_frases3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Eliminación de apostrofos\n",
    "\n",
    "El uso de apóstrofos es poco común en español; según la [Wikipedia](https://es.wikipedia.org/wiki/Apóstrofo):\n",
    "\n",
    ">En las lenguas que utilizan el alfabeto latino, el apóstrofo indica por lo general la elisión de una letra. También se utiliza para indicar uno o más fonemas suprimidos por razones métricas (en poesía, especialmente) o simulando una pronunciación dialectal: <br><br>\n",
    "Ya ciego con la vista’e la prenda, siguió nuestro hombre pa’l río y en llegando la vido que nadaba cerquita’e la orilla <br>(Güiraldes, Ricardo. Don Segundo Sombra, 1926)<br><br>\n",
    "Tales empleos son siempre arcaicos, pseudofonéticos o especializados, y el signo es raro en la escritura formal de uso corriente.\n",
    "\n",
    "Sin embargo, en otros [idiomas](https://en.wikipedia.org/wiki/Apostrophe), el apostrofo puede ser de uso común. En el caso del inglés su uso suele ser criticado y visto como poco formal, sin embargo, es un elemento muy común en el contexto no formal, como el provisto por las redes sociales.\n",
    "\n",
    "Lo mismo en inglés que en español, el apostrofo suele representar una redundancia en el idioma. El token \"*pa'l*\" ofrece la misma información que el par de tokens (\"para\", \"el\"). Lo mismo que en inglés, \"they're\" duplica la información ofrecida por (\"they\", \"are\"). De manera que es una buena práctica eliminar los términos con apóstrofos, reemplazándolos por su equivalente extendido:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "APOSTROFOS = {\"that's\" : \"that is\", \"they're\" : \"they are\"} # Se requiere construir un diccionario extenso \n",
    "words = texto4.split()\n",
    "texto5 = [APOSTROFOS[word] if word in APOSTROFOS else word for word in words]\n",
    "texto5 = \" \".join(texto5)\n",
    "print(texto5)\n",
    "\n",
    "df_frases4 = df_frases3.copy()   \n",
    "for i in range(len(df_frases4.index)):\n",
    "    words_row = [APOSTROFOS[word] if word in APOSTROFOS else word for word in df_frases4[\"Frase\"].iloc[i].split()]\n",
    "    df_frases4['Frase'].iloc[i] = \" \".join(words_row)\n",
    "\n",
    "display(df_frases4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Eliminar argot\n",
    "\n",
    "El argot es un lenguaje especializado utilizado para ocultar información (\"echarse un perico\" -¡no lo hagan!-), para definir identidad (\"¡ah'ñil!\" -> \"Por supuesto\"), para simplificar o abreviar una conversación (\"¡fierro!\" -> \"¡hagámoslo de inmediato!\"), como adopción de un término extranjero sin traducción natural (\"tokenizar\"), como aceptación de un término incorrecto (\"vocho\"), etc. \n",
    "\n",
    "Actualmente, con la proliferación de las redes sociales, con la posibilidad de publicar sin ninguna restricción, sin requisitos de respetar reglas ortográficas, sin correctores y con la prisa de contestar mensajes con la mayor simplicidad posible, el lenguaje escrito y disponible en la Web se ha plagado de términos que, no siendo correctos, podemos englobar dentro del rubro de lenguaje argot. Para simplificar el análisis del texto, eliminamos también estos términos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SLANG = {\"<3\" : \"love\", \"rofl\" : \"i laugh hysterically\", \"lmao\" : \"I get really happy\", \"&\" : \"and\"}\n",
    "words = texto5.split()\n",
    "texto6 = [SLANG[word] if word in SLANG else word for word in words]\n",
    "texto6 = \" \".join(texto6)\n",
    "print(texto6)\n",
    "\n",
    "df_frases5 = df_frases4.copy()   \n",
    "for i in range(len(df_frases5.index)):\n",
    "    words_row = [SLANG[word] if word in SLANG else word for word in df_frases5[\"Frase\"].iloc[i].split()]\n",
    "    df_frases5['Frase'].iloc[i] = \" \".join(words_row)\n",
    "display(df_frases5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Eliminar otros caracteres no alfabéticos\n",
    "\n",
    "Una vez que se han recuperado todas las palabras \"codificadas\" (como <3, por ejemplo), se eliminan los caracteres no alfabéticos restantes. Esto es simple con una expresión regular:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto7 = re.sub(\"[^a-zA-Z]\", \" \", texto6).split()\n",
    "texto7 = \" \".join(texto7)\n",
    "print(texto7)\n",
    "\n",
    "df_frases6 = df_frases5.copy()   \n",
    "df_frases6['Frase'] = [re.sub(\"[^a-zA-Z]\", \" \", word) for word in df_frases6[\"Frase\"]]\n",
    "display(df_frases6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Eliminación de palabras vacías\n",
    "\n",
    "Las palabras vacías (\"*[stop words](http://onlinelibrary.wiley.com/doi/10.1002/asi.5090110403/abstract;jsessionid=42107B5630F9A4EFA66CE2C72AB40934.f04t01)*\") son palabras que, debido a que tienen una alta frecuencia, distorsionan el análisis de un texto. \n",
    "\n",
    "Considérese, por ejemplo, el texto del cuento [Anaconda](http://www.cuentosinfin.com/anaconda/) de Horacio Quiroga. Las frecuencias de las palabras utilizadas en este texto son:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "puncSpList = [\".\", \";\", \":\", \"!\", \"¡\", \"¿\", \"?\", \"/\", \"-\", \"\\\\\", \",\", \")\", \"(\", \"\\\"\"]\n",
    "file = open(\"public literature/Anaconda.txt\", encoding='utf-8')\n",
    "anaconda = file.read()\n",
    "file.close()\n",
    "\n",
    "anaconda = anaconda.lower().split()\n",
    "for punc in puncSpList:\n",
    "    anaconda = [word.replace(punc,'') for word in anaconda]\n",
    "most_common_words = nltk.FreqDist(anaconda)\n",
    "\n",
    "print(\"Cantidad de palabras en el texto: \", most_common_words.N())\n",
    "print(\"\\nPalabras más populares:\\n\", most_common_words.most_common(50))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.figure(figsize=(15, 5))\n",
    "most_common_words.plot(50, cumulative=False)\n",
    "plt.figure(figsize=(15, 5))\n",
    "most_common_words.plot(50, cumulative=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puede observarse que las palabras con mayor frecuencia son palabras que no describen el tema del texto (\"de\", \"la\", \"a\"...). La primera palabra \"importante\" es \"*ñacaniná*\" (una serpiente) y tiene apena una frecuencia de 57 (comparada con 507 apariciones de la palabra \"de\". Las palabras previas acumulan más de 3700 palabras (de un total de 10067).\n",
    "\n",
    "Si ahora eliminamos las *palabras vacías*, obtenemos lo siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords # Importar la lista de palabras vacías\n",
    "sw = set(stopwords.words(\"spanish\"))\n",
    "\n",
    "anaconda = [w for w in anaconda if not w in sw]\n",
    "\n",
    "most_common_words = nltk.FreqDist(anaconda)\n",
    "\n",
    "print(\"Cantidad de palabras en el texto: \", most_common_words.N())\n",
    "print(\"\\nPalabras más populares:\\n\", most_common_words.most_common(50))\n",
    "\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "plt.figure(figsize=(15, 5))\n",
    "most_common_words.plot(50, cumulative=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, el conjunto de palabras sobresalientes es más descriptivo. \n",
    "\n",
    "A continuación, se eliminan las palabras vacías del texto de opinión de películas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = set(stopwords.words(\"english\"))\n",
    "\n",
    "words = texto7.split()\n",
    "texto8 = [w for w in words if not w in stops]   \n",
    "texto8 = \" \".join(texto8)\n",
    "print(texto8)\n",
    "\n",
    "df_frases7 = df_frases6.copy()   \n",
    "for i in range(len(df_frases7.index)):\n",
    "    words_row = [word for word in df_frases7[\"Frase\"].iloc[i].split() if not word in stops]\n",
    "    df_frases7['Frase'].iloc[i] = \" \".join(words_row)\n",
    "display(df_frases7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Otra vez, tokenización\n",
    "\n",
    "Con el texto limpio, volvemos a hacer la tokenización. Este es un conjunto de tokens adecuado para analizar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(texto8.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "\n",
    "En esta lección se analizaron las acciones generales para preparar un texto para ser analizado, particularmente en lo que se refiere a la limpieza de los datos, sin embargo, el proceso de preparación de los datos, es una tarea que debe adecuarse a cada problema y al entorno general del mismo. \n",
    "\n",
    "<hr style=\"border-width: 3px;\">\n",
    "\n",
    "### Tarea 2\n",
    "\n",
    "Seleccione un conjunto de textos para el problema de su elección y aplique los pasos de limpieza de texto y tokenización realizados en esta lección.\n",
    "\n",
    "**Fecha de entrega**: Martes 22 de agosto."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
